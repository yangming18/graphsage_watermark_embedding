{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Adam is not an Optimizer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 144\u001b[0m\n\u001b[0;32m    142\u001b[0m inner_net \u001b[38;5;241m=\u001b[39m GraphOpt( mtopo, mfeat, graphsage, device, inner_iter_num\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, T\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# call solve for inner-loop process to use objective to optimize inner model\u001b[39;00m\n\u001b[1;32m--> 144\u001b[0m optimal_inner_net,Sopt \u001b[38;5;241m=\u001b[39m \u001b[43minner_net\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms5_nodes_per_graph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minner model attained the stationary condition\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;66;03m# define the outer loss: cross entropy from optimized inner model prediction on dataset D\u001b[39;00m\n",
      "File \u001b[1;32md:\\soft\\anaconda\\envs\\graphsage\\lib\\site-packages\\torchopt\\diff\\implicit\\nn\\module.py:153\u001b[0m, in \u001b[0;36menable_implicit_gradients.<locals>.wrapped\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    150\u001b[0m params_names, flat_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mnamed_params))\n\u001b[0;32m    151\u001b[0m meta_params_names, flat_meta_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mnamed_meta_params))\n\u001b[1;32m--> 153\u001b[0m flat_optimal_params, output \u001b[38;5;241m=\u001b[39m \u001b[43mstateless_solver_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_meta_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmeta_params_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m swap_state(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mzip\u001b[39m(params_names, flat_optimal_params))\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32md:\\soft\\anaconda\\envs\\graphsage\\lib\\site-packages\\torchopt\\diff\\implicit\\decorator.py:429\u001b[0m, in \u001b[0;36m_custom_root.<locals>.wrapped_solver_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    426\u001b[0m args_signs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(args_signs)\n\u001b[0;32m    427\u001b[0m flat_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(flat_args)\n\u001b[1;32m--> 429\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmake_custom_vjp_solver_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43msolver_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_signs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;241m*\u001b[39moutput, aux, output_is_tensor, output_type \u001b[38;5;241m=\u001b[39m result\n\u001b[0;32m    431\u001b[0m output \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m output_is_tensor \u001b[38;5;28;01melse\u001b[39;00m output_type(output)\n",
      "File \u001b[1;32md:\\soft\\anaconda\\envs\\graphsage\\lib\\site-packages\\torch\\autograd\\function.py:574\u001b[0m, in \u001b[0;36mFunction.apply\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[0;32m    573\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[1;32m--> 574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[0;32m    577\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    578\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    582\u001b[0m     )\n",
      "File \u001b[1;32md:\\soft\\anaconda\\envs\\graphsage\\lib\\site-packages\\torchopt\\diff\\implicit\\decorator.py:297\u001b[0m, in \u001b[0;36m_custom_root.<locals>.make_custom_vjp_solver_fn.<locals>.ImplicitMetaGradient.forward\u001b[1;34m(ctx, *flat_args)\u001b[0m\n\u001b[0;32m    294\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(args)\n\u001b[0;32m    296\u001b[0m args, kwargs \u001b[38;5;241m=\u001b[39m _extract_kwargs(kwarg_keys, args)\n\u001b[1;32m--> 297\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43msolver_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(output) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n",
      "File \u001b[1;32md:\\soft\\anaconda\\envs\\graphsage\\lib\\site-packages\\torchopt\\diff\\implicit\\nn\\module.py:137\u001b[0m, in \u001b[0;36menable_implicit_gradients.<locals>.stateless_solver_fn\u001b[1;34m(flat_params, flat_meta_params, params_names, meta_params_names, self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;129m@custom_root\u001b[39m(_stateless_optimality_fn, argnums\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, has_aux\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msolve_kwargs)\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstateless_solver_fn\u001b[39m(\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;66;03m# pylint: disable=unused-argument\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    135\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[TupleOfTensors, Any]:\n\u001b[0;32m    136\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Solve the optimization problem.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mcls_solve\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m     flat_optimal_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(p\u001b[38;5;241m.\u001b[39mdetach_() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m flat_optimal_params, output\n",
      "Cell \u001b[1;32mIn[3], line 92\u001b[0m, in \u001b[0;36mGraphOpt.solve\u001b[1;34m(self, S, D, s5_nodes_per_graph)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# set up LinearLR\u001b[39;00m\n\u001b[0;32m     91\u001b[0m total_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minner_iter_num \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(data_loader)\n\u001b[1;32m---> 92\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m \u001b[43mLinearLR\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# Step2: start inner model training loop\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n",
      "File \u001b[1;32md:\\soft\\anaconda\\envs\\graphsage\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:733\u001b[0m, in \u001b[0;36mLinearLR.__init__\u001b[1;34m(self, optimizer, start_factor, end_factor, total_iters, last_epoch, verbose)\u001b[0m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_factor \u001b[38;5;241m=\u001b[39m end_factor\n\u001b[0;32m    732\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_iters \u001b[38;5;241m=\u001b[39m total_iters\n\u001b[1;32m--> 733\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\soft\\anaconda\\envs\\graphsage\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:93\u001b[0m, in \u001b[0;36mLRScheduler.__init__\u001b[1;34m(self, optimizer, last_epoch, verbose)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, optimizer: Optimizer, last_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# Attach optimizer\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(optimizer, Optimizer):\n\u001b[1;32m---> 93\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(optimizer)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not an Optimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m optimizer\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;66;03m# Initialize epoch and base learning rates\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: Adam is not an Optimizer"
     ]
    }
   ],
   "source": [
    "# Bilevel optimization\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchopt\n",
    "from torchopt.nn import ImplicitMetaGradientModule\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "from torch_geometric.loader import DataLoader\n",
    "from optimization_final_loss import Mtopo, Mfeat, GraphSAGE, SupervisedGraphSage\n",
    "import pickle\n",
    "\n",
    "# define the implicit gradient module\n",
    "class GraphOpt(ImplicitMetaGradientModule):\n",
    "    def __init__(self, mtopo, mfeat, graphsage, device, inner_iter_num=10, T=0.1, batch_size=32):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        # Meta-models\n",
    "        self.mtopo = mtopo.to(device)\n",
    "        self.mfeat = mfeat.to(device)\n",
    "        # Inner model\n",
    "        self.net = torchopt.module_clone(graphsage, by='deepcopy', detach_buffers=True)\n",
    "        # other hyperparameters\n",
    "        self.batch_size = batch_size\n",
    "        self.T = T\n",
    "        self.inner_iter_num = inner_iter_num\n",
    "\n",
    "    def forward(self, features, edge_index, batch):\n",
    "        ''' get the scores as node embeddings and graph embeddings used for\n",
    "    loss function calculation in objective() '''\n",
    "        scores, graph_embeds = self.net(features, edge_index, batch)\n",
    "\n",
    "        return scores, graph_embeds\n",
    "    \n",
    "    def objective(self, features, edge_index, batch, labels, group_labels):\n",
    "        \"\"\"\n",
    "        Define the loss function for the inner-loop optimization using final_loss().\n",
    "        \"\"\"\n",
    "        scores, graph_embeds = self(features, edge_index, batch)\n",
    "        # step1: get the cross-Entropy Loss\n",
    "        labels = labels.long() # converting the tensor labels to the torch.int64 data type, also known as long in PyTorch.\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        classification_loss = loss_fn(scores, labels)\n",
    "        # step2: Compute SNNL Loss\n",
    "        snnl_loss = self.net.snnl_loss(graph_embeds, group_labels, self.T)\n",
    "        # step3: Combine Cross-Entropy Loss and SNNL Loss\n",
    "        loss = classification_loss - snnl_loss\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def solve(self, S, D, s5_nodes_per_graph):\n",
    "        '''\n",
    "        use meta model mtopo and mfeat to modify S, which incorporates meta model into the computational graph.\n",
    "        start inner model training using the self.objective as loss.\n",
    "        return the optimized inner model and modified dataset Sopt.\n",
    "        '''\n",
    "        Sopt = []\n",
    "        # Step 1: before inner training loop, modify S to get Sopt using Mtopo and Mfeat\n",
    "        for idx, data in enumerate(S):\n",
    "            data = data.to(self.device)\n",
    "            s5_nodes = s5_nodes_per_graph[idx]  # Indices of the 5 nodes to modify\n",
    "            num_nodes = data.num_nodes\n",
    "            edge_index = data.edge_index\n",
    "            A = torch.zeros((num_nodes, num_nodes), device=self.device)\n",
    "            A[edge_index[0], edge_index[1]] = 1.0\n",
    "\n",
    "            # Extract subgraph adjacency and features\n",
    "            A_s5 = A[s5_nodes][:, s5_nodes].unsqueeze(0)  # Shape: [1, 5, 5]\n",
    "            X_s5 = data.x[s5_nodes].unsqueeze(0)          # Shape: [1, 5, K]\n",
    "\n",
    "            # Modify topology and features using Mtopo and Mfeat\n",
    "            A_s5_tilde = self.mtopo(A_s5)  # Modified adjacency\n",
    "            X_s5_tilde = self.mfeat(X_s5)  # Modified features\n",
    "\n",
    "            # Update adjacency matrix and features\n",
    "            A[s5_nodes][:, s5_nodes] = A_s5_tilde.squeeze(0)\n",
    "            data.x[s5_nodes] = X_s5_tilde.squeeze(0)\n",
    "\n",
    "            # Reconstruct edge_index from updated adjacency matrix\n",
    "            A_binary = (A > 0.5).nonzero(as_tuple=False).t()\n",
    "            data.edge_index = A_binary\n",
    "\n",
    "            Sopt.append(data)\n",
    "\n",
    "        # combine the normal dataset D and modified watermark dataset Sopt\n",
    "        combined_graph_list = D + Sopt\n",
    "        data_loader = DataLoader(combined_graph_list, batch_size=self.batch_size, shuffle=True)\n",
    "        # prepare for inner model training loop\n",
    "        params = tuple(self.parameters())\n",
    "        inner_optimizer = torchopt.Adam(params,lr=0.0005, weight_decay=5e-4 )\n",
    "        # set up LinearLR\n",
    "        total_steps = self.inner_iter_num * len(data_loader)\n",
    "        scheduler = LinearLR(inner_optimizer, start_factor=1.0, end_factor=0.1, total_iters=total_steps)\n",
    "        # Step2: start inner model training loop\n",
    "        with torch.enable_grad():\n",
    "            for i in range(self.inner_iter_num):\n",
    "                self.net.train()\n",
    "                total_loss = 0\n",
    "                for batch in data_loader:\n",
    "                    batch = batch.to(self.device)\n",
    "                    features = batch.x\n",
    "                    edge_index = batch.edge_index\n",
    "                    labels = batch.y\n",
    "                    batch_graph_indices = batch.batch\n",
    "                    group_labels = batch.group_label\n",
    "                    inner_optimizer.zero_grad()\n",
    "                    loss = self.objective(features, edge_index, batch_graph_indices, labels, group_labels)\n",
    "                    loss.backward(inputs=params)\n",
    "                    inner_optimizer.step()\n",
    "                    scheduler.step()\n",
    "                    total_loss += loss.item() \n",
    "                avg_loss = total_loss / len(data_loader)\n",
    "                print(f\"Epoch {i + 1}/{self.inner_iter_num}, Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        return self, Sopt\n",
    "\n",
    "# Set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# load the necessary datasets\n",
    "with open('new_keyinut_dataset_enzymes.pkl', 'rb') as f:\n",
    "    S = pickle.load(f)\n",
    "with open('train_dataset_enzymes.pkl', 'rb') as f:\n",
    "    D = pickle.load(f)\n",
    "# add different group labels for different dataset\n",
    "for graph in D:\n",
    "    graph.group_label = torch.tensor(0, dtype=torch.long)\n",
    "for graph in S:\n",
    "    graph.group_label = torch.tensor(1, dtype=torch.long)\n",
    "# load 5 nodes storage information per each graph in dataset S\n",
    "with open('s5_nodes_per_graph.pkl', 'rb') as f:\n",
    "    s5_nodes_per_graph = pickle.load(f)  # List of lists, one per graph\n",
    "# initialize the inner model graphsage\n",
    "input_dim = 18\n",
    "hidden_dims = [128, 128]\n",
    "num_classes = 6\n",
    "graphsage_model = GraphSAGE(input_dim, hidden_dims, num_sample=10, gcn=False)\n",
    "graphsage = SupervisedGraphSage(num_classes, graphsage_model, readout=\"sum\")\n",
    "# initialize the meta model Mtopo and Mfeat\n",
    "mtopo = Mtopo()\n",
    "mfeat = Mfeat(feature_dim=18)\n",
    "\n",
    "# initialize the implicit module\n",
    "inner_net = GraphOpt( mtopo, mfeat, graphsage, device, inner_iter_num=10, T=0.1, batch_size=32)\n",
    "# call solve for inner-loop process to use objective to optimize inner model\n",
    "optimal_inner_net,Sopt = inner_net.solve(S, D, s5_nodes_per_graph)\n",
    "print(\"inner model attained the stationary condition'\\n'\")\n",
    "\n",
    "# define the outer loss: cross entropy from optimized inner model prediction on dataset D\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "D_loader = DataLoader(D, batch_size=32, shuffle=False)\n",
    "outer_loss = 0\n",
    "for batch in D_loader:\n",
    "    batch = batch.to(device)\n",
    "    features = batch.x\n",
    "    edge_index = batch.edge_index\n",
    "    D_labels = batch.y\n",
    "    batch_graph_indices = batch.batch\n",
    "    # get the each graph prediction scores\n",
    "    D_scores,_ = optimal_inner_net(features, edge_index, batch)\n",
    "    # compute the outer loss\n",
    "    loss = loss_fn(D_scores, D_labels)\n",
    "    outer_loss += loss.item()\n",
    "outer_loss = outer_loss/len(D_loader)\n",
    "\n",
    "# Derive the meta-gradient\n",
    "torch.autograd.grad(outer_loss, mtopo.parameters())\n",
    "torch.autograd.grad(outer_loss, mfeat.parameters())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphsage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
